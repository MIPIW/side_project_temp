{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from vllm import LLM, SamplingParams, EngineArgs\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    GenerationConfig,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "import importlib\n",
    "\n",
    "import vllm\n",
    "\n",
    "importlib.reload(vllm)\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "import transformers\n",
    "\n",
    "importlib.reload(transformers)\n",
    "from transformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['|Device 0| Mem Free: 81082.38MB / 81920.00MB(used: 837.62MB, 1.0%)']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DeviceUtil:\n",
    "    @staticmethod\n",
    "    def gpu_usage():\n",
    "        try:\n",
    "            import nvidia_smi\n",
    "\n",
    "            nvidia_smi.nvmlInit()\n",
    "            deviceCount = nvidia_smi.nvmlDeviceGetCount()\n",
    "            lst = []\n",
    "            for i in range(deviceCount):\n",
    "                handle = nvidia_smi.nvmlDeviceGetHandleByIndex(i)\n",
    "                util = nvidia_smi.nvmlDeviceGetUtilizationRates(handle)\n",
    "                mem = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "                lst.append(\n",
    "                    f\"|Device {i}| Mem Free: {mem.free/1024**2:5.2f}MB / {mem.total/1024**2:5.2f}MB\"\n",
    "                    + f\"(used: {(mem.total/1024**2 - mem.free/1024**2):5.2f}MB, {round((mem.total/1024**2 - mem.free/1024**2) / (mem.total/1024**2) * 100, 1)}%)\",\n",
    "                )\n",
    "            return lst\n",
    "\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "    @staticmethod\n",
    "    def empty_gpu(*, args_list, delete=False):\n",
    "        import gc, time\n",
    "\n",
    "        for i in args_list:\n",
    "            if delete:\n",
    "                del i\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        for i in range(5):\n",
    "            time.sleep(1)\n",
    "            print(f\"{i} of 5\")\n",
    "        DeviceUtil.gpu_usage()\n",
    "\n",
    "\n",
    "class ModelUtil:\n",
    "    import torch\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        model_checkpoint,\n",
    "        model_loading_class,\n",
    "        tokenizer_loading_class,\n",
    "        tokenizer_checkpoint=None,\n",
    "        cpu_only=False,\n",
    "        to_device=True,\n",
    "    ):\n",
    "        self.model = model_loading_class.from_pretrained(model_checkpoint)\n",
    "        count_params = sum(p.numel() for p in self.model.parameters())\n",
    "        print(\"Total Parameters: \", \"{:,}\".format(count_params))\n",
    "\n",
    "        if tokenizer_checkpoint == None:\n",
    "            self.tokenizer = tokenizer_loading_class.from_pretrained(model_checkpoint)\n",
    "        else:\n",
    "            self.tokenizer = tokenizer_loading_class.from_pretrained(\n",
    "                tokenizer_checkpoint\n",
    "            )\n",
    "\n",
    "        print(\"Before: \")\n",
    "        DeviceUtil.gpu_usage()\n",
    "\n",
    "        self.device = (\n",
    "            \"cuda\" if (cpu_only == False) and torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        if to_device == True:\n",
    "            self.model.to(self.device)\n",
    "\n",
    "        print(\"After: \")\n",
    "        DeviceUtil.gpu_usage()\n",
    "\n",
    "\n",
    "DeviceUtil.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# temperature=0.8, top_p=0.95,\n",
    "\n",
    "\n",
    "class PagedAttention:\n",
    "    def __init__(self, checkpoint):\n",
    "        self.checkpoint = checkpoint\n",
    "\n",
    "    def generate(self, prompts, batch, generationconfig):\n",
    "        top_k = generationconfig.pop(\"top_k\")\n",
    "        n = generationconfig.pop(\"n\")\n",
    "        max_tokens = generationconfig.pop(\"max_tokens\")\n",
    "        min_tokens = generationconfig.pop(\"min_tokens\")\n",
    "        seed = generationconfig.pop(\"seed\")\n",
    "        temperature = generationconfig.pop(\"temperature\")\n",
    "\n",
    "        sampling_params = SamplingParams(\n",
    "            top_k=top_k,\n",
    "            temperature=temperature,\n",
    "            n=n,\n",
    "            max_tokens=max_tokens,\n",
    "            min_tokens=min_tokens,\n",
    "            seed=seed,\n",
    "        )\n",
    "\n",
    "        # EngineArgs(self.checkpoint, max_model_len=max_tokens)\n",
    "\n",
    "        s1 = time.time()\n",
    "        llm = LLM(model=self.checkpoint)\n",
    "        print(llm.llm_engine.scheduler_config.max_model_len)\n",
    "\n",
    "        s2 = time.time()\n",
    "        lst = []\n",
    "\n",
    "        if batch:\n",
    "            outputs = llm.generate(prompts, sampling_params=sampling_params)\n",
    "\n",
    "            lst = []\n",
    "            for output in outputs:\n",
    "                prompt: str = output.prompt\n",
    "                generated_text = output.outputs[0].text\n",
    "                lst.append(prompt + generated_text)\n",
    "\n",
    "            s3 = time.time()\n",
    "\n",
    "            return lst, s3 - s2, DeviceUtil.gpu_usage()\n",
    "\n",
    "        else:\n",
    "            for i in prompts:\n",
    "                outputs = llm.generate(i, sampling_params=sampling_params)\n",
    "\n",
    "                for output in outputs:\n",
    "                    prompt: str = output.prompt\n",
    "                    generated_text = output.outputs[0].text\n",
    "                    lst.append(prompt + generated_text)\n",
    "\n",
    "            s3 = time.time()\n",
    "            return lst, s3 - s2, DeviceUtil.gpu_usage()\n",
    "\n",
    "\n",
    "class NormalGenerator:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "            attn_implementation=\"flash_attention_2\",\n",
    "        )  # , token = \"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\"\n",
    "        self.model.half()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "        )  # , token = \"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\")\n",
    "\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def generate(self, prompts, batch, generationconfig):\n",
    "\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        top_k = generationconfig.pop(\"top_k\")\n",
    "        n = generationconfig.pop(\"n\")\n",
    "        max_tokens = generationconfig.pop(\"max_tokens\")\n",
    "        min_tokens = generationconfig.pop(\"min_tokens\")\n",
    "        do_sample = generationconfig.pop(\"do_sample\")\n",
    "\n",
    "        if batch:\n",
    "\n",
    "            s = time.time()\n",
    "\n",
    "            tokenized_chat = self.tokenizer(\n",
    "                prompts, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "            ).to(self.device)\n",
    "\n",
    "            lst = self.model.generate(\n",
    "                **tokenized_chat,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                do_sample=do_sample,\n",
    "                min_length=min_tokens,\n",
    "                max_length=max_tokens,\n",
    "            )\n",
    "            lst = self.tokenizer.batch_decode(lst, skip_special_tokens=True)\n",
    "\n",
    "            s1 = time.time()\n",
    "\n",
    "            return lst, s1 - s, DeviceUtil.gpu_usage()\n",
    "\n",
    "        else:\n",
    "\n",
    "            s = time.time()\n",
    "\n",
    "            lst = []\n",
    "            for i in prompts:\n",
    "                tokenized_chat = self.tokenizer(\n",
    "                    i, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "                ).to(self.device)\n",
    "\n",
    "                out = self.model.generate(\n",
    "                    **tokenized_chat,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    do_sample=do_sample,\n",
    "                    min_length=min_tokens,\n",
    "                    max_length=max_tokens,\n",
    "                )\n",
    "                out = self.tokenizer.batch_decode(out, skip_special_tokens=True)\n",
    "                lst.append(out[0])\n",
    "\n",
    "            s1 = time.time()\n",
    "\n",
    "            return lst, s1 - s, DeviceUtil.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def my_seed(seed=42):\n",
    "#     import numpy as np\n",
    "#     import random\n",
    "#     import os\n",
    "\n",
    "#     np.random.seed(seed)\n",
    "#     random.seed(seed)\n",
    "#     torch.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed(seed)\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "#     torch.backends.cudnn.benchmark = False\n",
    "#     os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "\n",
    "def main(prompts, machine_type, batched, generationconfig):\n",
    "\n",
    "    x = DeviceUtil.gpu_usage()\n",
    "    checkpoint = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "    # generationconfig = GenerationConfig.from_pretrained(\n",
    "    #     checkpoint, max_length=128, min_lenght=128, do_sample=False\n",
    "    # )\n",
    "\n",
    "    if machine_type == \"vLLM\":\n",
    "        pagedattention = PagedAttention(checkpoint)\n",
    "        y = DeviceUtil.gpu_usage()\n",
    "\n",
    "        output = pagedattention.generate(prompts, batched, generationconfig)\n",
    "\n",
    "    elif machine_type == \"regular\":\n",
    "        gen = NormalGenerator()\n",
    "        y = DeviceUtil.gpu_usage()\n",
    "\n",
    "        output = gen.generate(prompts, batched, generationconfig)\n",
    "\n",
    "    else:\n",
    "        y = []\n",
    "        output = []\n",
    "\n",
    "    output = x + y + list(output)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyohyeongjang/.conda/envs/hyohyeongjang_base/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-09 23:04:31 llm_engine.py:161] Initializing an LLM engine (v0.5.0.post1) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention backend:  _Backend.FLASH_ATTN\n",
      "attention backend:  _Backend.FLASH_ATTN\n",
      "INFO 07-09 23:04:32 weight_utils.py:218] Using model weights format ['*.safetensors']\n",
      "INFO 07-09 23:04:41 model_runner.py:160] Loading model weights took 14.9595 GB\n",
      "gpu 0.9\n",
      "gpu_memory: free, total, cache, gpu_utilization, num_cpu_blocks (59444166656, 85024112640, 2097152, 0.9, 24499)\n",
      "INFO 07-09 23:04:43 gpu_executor.py:83] # GPU blocks: 24499, # CPU blocks: 2048\n",
      "kv_cache_shape: 2, num_blocks, block_size, num_kv_heads, head_size (2, 24499, 16, 8, 128)\n",
      "is_pin_memory_available: False\n",
      "kv_cache_shape: 2, num_blocks, block_size, num_kv_heads, head_size (2, 2048, 16, 8, 128)\n",
      "is_pin_memory_available: True\n",
      "INFO 07-09 23:04:46 model_runner.py:889] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 07-09 23:04:46 model_runner.py:893] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 07-09 23:04:56 model_runner.py:965] Graph capturing finished in 9 secs.\n",
      "8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%| | 0/102"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 07-09 23:06:58 scheduler.py:1089] Sequence group 1023 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n",
      "WARNING 07-09 23:07:10 scheduler.py:1089] Sequence group 973 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=51\n",
      "WARNING 07-09 23:07:16 scheduler.py:1089] Sequence group 923 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=101\n",
      "WARNING 07-09 23:07:28 scheduler.py:1089] Sequence group 873 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=151\n",
      "WARNING 07-09 23:07:33 scheduler.py:1089] Sequence group 823 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=201\n",
      "WARNING 07-09 23:07:43 scheduler.py:1089] Sequence group 773 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 1024/\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    set_seed(42)\n",
    "\n",
    "    prompts = [\"The biggest challenge we face is\"] * 765\n",
    "\n",
    "    # generationconfig_regular = {\n",
    "    #     \"max_tokens\": 512,\n",
    "    #     \"min_tokens\": 512,\n",
    "    #     \"do_sample\": False,\n",
    "    #     \"top_k\": 1,\n",
    "    #     \"n\": 1,\n",
    "    #     \"seed\": 42,\n",
    "    #     \"temperature\": 1e-6,\n",
    "    #     \"top_p\": None,\n",
    "    # }\n",
    "\n",
    "    # output_regular = main(\n",
    "    #     prompts=prompts,\n",
    "    #     machine_type=\"regular\",\n",
    "    #     batched=True,\n",
    "    #     generationconfig=generationconfig_regular,\n",
    "    # )\n",
    "\n",
    "    generationconfig_vllm = {\n",
    "        \"max_tokens\": 505,\n",
    "        \"min_tokens\": 505,\n",
    "        \"do_sample\": False,\n",
    "        \"top_k\": 1,\n",
    "        \"temperature\": 1,\n",
    "        \"n\": 1,\n",
    "        \"seed\": 42,\n",
    "    }\n",
    "\n",
    "    output_vllm = main(\n",
    "        prompts=prompts,\n",
    "        machine_type=\"vLLM\",\n",
    "        batched=True,\n",
    "        generationconfig=generationconfig_vllm,\n",
    "    )\n",
    "\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "    checkpoint = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\n",
    "    tokenizer.encode(\n",
    "        \"... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ...\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\n",
    "    \"... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ...\".split(\n",
    "        \" \"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "765.59375"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "24499 * 16 / 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'total blocks: 24499, blocks required: 32768.0, unsaturated slots: 5584.5'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "x = Counter([i for j in [i.split(\" \") for i in output_vllm[2]] for i in j])\n",
    "x[\"...\"]\n",
    "\n",
    "f\"total blocks: {24499}, blocks required: {1024 * 512 / 16}, unsaturated slots: {89352 / 16}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5584.5"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89352"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/hyohyeongjang/.conda/envs/hyohyeongjang_base/lib/python3.9/site-packages/vllm/core/scheduler.py'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### FileUtils에 추가하기\n",
    "import os\n",
    "\n",
    "\n",
    "def find(name, path):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        if name in files:\n",
    "            return os.path.join(root, name)\n",
    "\n",
    "\n",
    "find(\n",
    "    \"scheduler.py\",\n",
    "    \"/home/hyohyeongjang/.conda/envs/hyohyeongjang_base/lib/python3.9/site-packages/vllm\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.048828125"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(67845 - 17619) / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4 * 128 * 2 * 4096 * 32 * 2 / (1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1824 * 32 / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3844de5c8d464c1caff9a9ee745ea819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8030261248"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
