{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, importlib, time, sys\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
    "from os import path\n",
    "import os\n",
    "\n",
    "utils_path = path.abspath(path.join(os.path.abspath(\"\"), \"../myUtils\"))\n",
    "sys.path.append(utils_path)\n",
    "import deviceUtils, IOUtils, checkUtils, modelUtils, parallelUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AutoModelForCausalLM.from_pretrained(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temperature=0.8, top_p=0.95,\n",
    "\n",
    "\n",
    "class PagedAttention:\n",
    "    def __init__(self, checkpoint):\n",
    "        self.checkpoint = checkpoint\n",
    "\n",
    "    def generate(self, prompts, batch, generationconfig):\n",
    "        top_k = generationconfig.pop(\"top_k\")\n",
    "        n = generationconfig.pop(\"n\")\n",
    "        max_tokens = generationconfig.pop(\"max_tokens\")\n",
    "        min_tokens = generationconfig.pop(\"min_tokens\")\n",
    "        seed = generationconfig.pop(\"seed\")\n",
    "        temperature = generationconfig.pop(\"temperature\")\n",
    "\n",
    "        sampling_params = SamplingParams(\n",
    "            top_k=top_k,\n",
    "            temperature=temperature,\n",
    "            n=n,\n",
    "            max_tokens=max_tokens,\n",
    "            min_tokens=min_tokens,\n",
    "            seed=seed,\n",
    "        )\n",
    "\n",
    "        # EngineArgs(self.checkpoint, max_model_len=max_tokens)\n",
    "\n",
    "        s1 = time.time()\n",
    "        llm = LLM(model=self.checkpoint)\n",
    "        print(llm.llm_engine.scheduler_config.max_model_len)\n",
    "\n",
    "        s2 = time.time()\n",
    "        lst = []\n",
    "\n",
    "        if batch:\n",
    "            outputs = llm.generate(prompts, sampling_params=sampling_params)\n",
    "\n",
    "            lst = []\n",
    "            for output in outputs:\n",
    "                prompt: str = output.prompt\n",
    "                generated_text = output.outputs[0].text\n",
    "                lst.append(prompt + generated_text)\n",
    "\n",
    "            s3 = time.time()\n",
    "\n",
    "            return lst, s3 - s2, DeviceUtil.gpu_usage()\n",
    "\n",
    "        else:\n",
    "            for i in prompts:\n",
    "                outputs = llm.generate(i, sampling_params=sampling_params)\n",
    "\n",
    "                for output in outputs:\n",
    "                    prompt: str = output.prompt\n",
    "                    generated_text = output.outputs[0].text\n",
    "                    lst.append(prompt + generated_text)\n",
    "\n",
    "            s3 = time.time()\n",
    "            return lst, s3 - s2, DeviceUtil.gpu_usage()\n",
    "\n",
    "\n",
    "class NormalGenerator:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "            attn_implementation=\"flash_attention_2\",\n",
    "        )  # , token = \"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\"\n",
    "        self.model.half()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "        )  # , token = \"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\")\n",
    "\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def generate(self, prompts, batch, generationconfig):\n",
    "\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        top_k = generationconfig.pop(\"top_k\")\n",
    "        n = generationconfig.pop(\"n\")\n",
    "        max_tokens = generationconfig.pop(\"max_tokens\")\n",
    "        min_tokens = generationconfig.pop(\"min_tokens\")\n",
    "        do_sample = generationconfig.pop(\"do_sample\")\n",
    "\n",
    "        if batch:\n",
    "\n",
    "            s = time.time()\n",
    "\n",
    "            tokenized_chat = self.tokenizer(\n",
    "                prompts, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "            ).to(self.device)\n",
    "\n",
    "            lst = self.model.generate(\n",
    "                **tokenized_chat,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                do_sample=do_sample,\n",
    "                min_length=min_tokens,\n",
    "                max_length=max_tokens,\n",
    "            )\n",
    "            lst = self.tokenizer.batch_decode(lst, skip_special_tokens=True)\n",
    "\n",
    "            s1 = time.time()\n",
    "\n",
    "            return lst, s1 - s, DeviceUtil.gpu_usage()\n",
    "\n",
    "        else:\n",
    "\n",
    "            s = time.time()\n",
    "\n",
    "            lst = []\n",
    "            for i in prompts:\n",
    "                tokenized_chat = self.tokenizer(\n",
    "                    i, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "                ).to(self.device)\n",
    "\n",
    "                out = self.model.generate(\n",
    "                    **tokenized_chat,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    do_sample=do_sample,\n",
    "                    min_length=min_tokens,\n",
    "                    max_length=max_tokens,\n",
    "                )\n",
    "                out = self.tokenizer.batch_decode(out, skip_special_tokens=True)\n",
    "                lst.append(out[0])\n",
    "\n",
    "            s1 = time.time()\n",
    "\n",
    "            return lst, s1 - s, DeviceUtil.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def my_seed(seed=42):\n",
    "#     import numpy as np\n",
    "#     import random\n",
    "#     import os\n",
    "\n",
    "#     np.random.seed(seed)\n",
    "#     random.seed(seed)\n",
    "#     torch.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed(seed)\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "#     torch.backends.cudnn.benchmark = False\n",
    "#     os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "\n",
    "def main(prompts, machine_type, batched, generationconfig):\n",
    "\n",
    "    x = DeviceUtil.gpu_usage()\n",
    "    checkpoint = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "    # generationconfig = GenerationConfig.from_pretrained(\n",
    "    #     checkpoint, max_length=128, min_lenght=128, do_sample=False\n",
    "    # )\n",
    "\n",
    "    if machine_type == \"vLLM\":\n",
    "        pagedattention = PagedAttention(checkpoint)\n",
    "        y = DeviceUtil.gpu_usage()\n",
    "\n",
    "        output = pagedattention.generate(prompts, batched, generationconfig)\n",
    "\n",
    "    elif machine_type == \"regular\":\n",
    "        gen = NormalGenerator()\n",
    "        y = DeviceUtil.gpu_usage()\n",
    "\n",
    "        output = gen.generate(prompts, batched, generationconfig)\n",
    "\n",
    "    else:\n",
    "        y = []\n",
    "        output = []\n",
    "\n",
    "    output = x + y + list(output)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DeviceUtil' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 35\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# generationconfig_regular = {\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#     \"max_tokens\": 512,\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#     \"min_tokens\": 512,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#     generationconfig=generationconfig_regular,\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m     25\u001b[0m generationconfig_vllm \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m505\u001b[39m,\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m505\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m42\u001b[39m,\n\u001b[1;32m     33\u001b[0m }\n\u001b[0;32m---> 35\u001b[0m output_vllm \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmachine_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvLLM\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerationconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerationconfig_vllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[1;32m     44\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Meta-Llama-3-8B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[4], line 17\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(prompts, machine_type, batched, generationconfig)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m(prompts, machine_type, batched, generationconfig):\n\u001b[0;32m---> 17\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mDeviceUtil\u001b[49m\u001b[38;5;241m.\u001b[39mgpu_usage()\n\u001b[1;32m     18\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Meta-Llama-3-8B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# generationconfig = GenerationConfig.from_pretrained(\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m#     checkpoint, max_length=128, min_lenght=128, do_sample=False\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# )\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DeviceUtil' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    set_seed(42)\n",
    "\n",
    "    prompts = [\"The biggest challenge we face is\"] * 768\n",
    "\n",
    "    # generationconfig_regular = {\n",
    "    #     \"max_tokens\": 512,\n",
    "    #     \"min_tokens\": 512,\n",
    "    #     \"do_sample\": False,\n",
    "    #     \"top_k\": 1,\n",
    "    #     \"n\": 1,\n",
    "    #     \"seed\": 42,\n",
    "    #     \"temperature\": 1e-6,\n",
    "    #     \"top_p\": None,\n",
    "    # }\n",
    "\n",
    "    # output_regular = main(\n",
    "    #     prompts=prompts,\n",
    "    #     machine_type=\"regular\",\n",
    "    #     batched=True,\n",
    "    #     generationconfig=generationconfig_regular,\n",
    "    # )\n",
    "\n",
    "    generationconfig_vllm = {\n",
    "        \"max_tokens\": 505,\n",
    "        \"min_tokens\": 505,\n",
    "        \"do_sample\": False,\n",
    "        \"top_k\": 1,\n",
    "        \"temperature\": 1,\n",
    "        \"n\": 1,\n",
    "        \"seed\": 42,\n",
    "    }\n",
    "\n",
    "    output_vllm = main(\n",
    "        prompts=prompts,\n",
    "        machine_type=\"vLLM\",\n",
    "        batched=True,\n",
    "        generationconfig=generationconfig_vllm,\n",
    "    )\n",
    "\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "    checkpoint = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\n",
    "    tokenizer.encode(\n",
    "        \"... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ...\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\n",
    "    \"... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ...\".split(\n",
    "        \" \"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "24499 * 16 / 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "x = Counter([i for j in [i.split(\" \") for i in output_vllm[2]] for i in j])\n",
    "x[\"...\"]\n",
    "\n",
    "f\"total blocks: {24499}, blocks required: {1024 * 512 / 16}, unsaturated slots: {89352 / 16}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### FileUtils에 추가하기\n",
    "import os\n",
    "\n",
    "\n",
    "def find(name, path):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        if name in files:\n",
    "            return os.path.join(root, name)\n",
    "\n",
    "\n",
    "find(\n",
    "    \"scheduler.py\",\n",
    "    \"/home/hyohyeongjang/.conda/envs/hyohyeongjang_base/lib/python3.9/site-packages/vllm\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(67845 - 17619) / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4 * 128 * 2 * 4096 * 32 * 2 / (1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1824 * 32 / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
